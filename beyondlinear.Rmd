---
title: "Non linear modeling"
output:
  pdf_document: default
  html_notebook: default
---
```{r}
options(warn=-1)
```

In this notebook we'll use the Wage data from the ISLR library to explore the realm of non linear models.
```{r}
library (ISLR)
attach (Wage)
```
```{r}
head(Wage)
```

## Polynomial models 
Is the wage a 4 order polynomial of the age of the person, considering Gaussian noise in it? 
```{r}
fit=lm(wage~poly(age ,4) ,data=Wage)
summary(fit)
```
The answer to the above question seems to be partially positive, because the four order polynomial doesn't seem statistically significant to predict the response. Let's use cross-validation to evaluate the different models.
```{r}
ncv <- 5 
n <- dim(Wage)[1]
#shuffling
indices <- sample(1:n, size = n, replace=F)
#splitting
folds <- cut(indices, breaks = ncv, labels = F)
#poly order 
od <- c(1,2,3,4,5,6)
res <- matrix(nrow=ncv, ncol=6)
for(order in od){
  for(i in 1:ncv){
    test <- indices[folds==i]
    fit<-lm(wage~poly(age ,order) ,data=Wage, subset=-test)
    preds<-predict(fit, newdata=Wage[test,])
    error<-sum((preds-Wage$wage[test])**2)/length(test)
    res[i,order]<-error
  }
}
plot(res[,1], type="l", lty="dashed", col=2, ylim =c(min(res),max(res)), ylab="MSE")
lines(res[,2], lty="dashed", col=3)
lines(res[,3], lty="dashed", col=4)
lines(res[,4], lty="dashed", col=5)
lines(res[,5], lty="dashed", col=6)
lines(res[,6], lty="dashed", col=7)
legend("topright",legend=c("1","2","3","4","5","6"), col=c(2,3,4,5,6,7), lty="dashed")
```
```{r}
colMeans(res)
which.min(colMeans(res))
```
The fourth order degree seems to be the best one according to our cross validation! Let's see what the glm automatic cross validation would say. 
```{r}
library(boot)
res.glm <- numeric(6)
for(order in od){
  fit.glm <- glm(wage~poly(age ,order) ,data=Wage)
  res.glm[order] <- cv.glm(Wage, fit.glm, K=ncv)$delta[2]
}
res.glm
which.min(res.glm)
```
The glm cross-validation and our cross validation seem to agree. What about the ANOVA test? 
```{r}
fit1 <- lm(wage~poly(age ,1) ,data=Wage)
fit2 <- lm(wage~poly(age ,2) ,data=Wage)
fit3 <- lm(wage~poly(age ,3) ,data=Wage)
fit4 <- lm(wage~poly(age ,4) ,data=Wage)
fit5 <- lm(wage~poly(age ,5) ,data=Wage)
fit6 <- lm(wage~poly(age ,6) ,data=Wage)
anova(fit1,fit2,fit3,fit4,fit5,fit6)
```
Note that the p-values obtained with the ANOVA are the same we obtain from the T-test in the biggest model.
```{r}
summary(fit6)
```
This happens because the poly function automatically builds orthogonal coordinates, hence the p-value associated with one predictor cannot be influenced by the presence/absence of other predictors. 
The Anova hence, like the T-test, doesn't see the fourth order term as statistically significant. 
Let's have a look at the third and fourth order fits. 
```{r}
plot(Wage$age,Wage$wage, col="darkgray")
agelims <- range(Wage$age)
age.grid <- seq(from=agelims[1],to=agelims[2])
preds3 <- predict(fit3, newdata = data.frame(age=age.grid))
preds4 <- predict(fit4, newdata = data.frame(age=age.grid))
lines(age.grid, preds3, col="blue",lwd=2)
lines(age.grid, preds4, col="red",lwd=2)

```

## Step functions 
We now want to fit a step function to predict wage using age, and perform cross-validation to choose the optimal number of cuts. 
```{r}
# The number of cuts we want to experiment with
ncuts <- c(2,3,4,5,6,7,8,9,10)
res <- numeric(length(ncuts))
for(j in 1:length(ncuts)){
  nc <- ncuts[j]
  # saving the new factor variable in the dataframe
  Wage$age.cut <- cut(age,nc)
  # fit step function to the train data 
  fit.step <- glm(wage~age.cut, data=Wage)
  # evaluate the fit on the test data
  cv.res <- cv.glm(Wage, fit.step, K=ncv)$delta[2]
  res[j] <- cv.res
  
}

res
```
```{r}
plot(ncuts, res, type="l", lty="dashed", col="red")
points(ncuts[which.min((res))], min(res), cex=2)
```
Hence the minimum is obtained with 8 cuts. Let's have a look at the fitted function. 
```{r}
fit.8.steps <- glm(wage~cut(age,8), data=Wage)
preds.steps <- predict(fit.8.steps, newdata=data.frame(age=age.grid))
plot(age, wage, col="darkgray")
lines(age.grid, preds.steps, lwd=2, col="darkgreen")
```
Let's now explore other the non-linear relationships between wage and other variables.
```{r}
p <- dim(Wage)[2]
p
head(Wage)
```
```{r}
# 11 predictors 
pairs(Wage)
```
We'll now build a generalized additive model to predict the Wage based on the age, the marital status and the eductation. 
We'll first do some exploratory analysis to evaluate the relationships between wage and marital status and education. 
```{r}
plot(maritl,wage, xlab="maritl", ylab="wage")
```
Since the main difference seems to be between never married, married and the the other 3 categories together let's try models where we use this categorical variable with some of the levels. 
```{r}
lvl1 <- maritl == levels(as.factor(maritl))[1]
lvl2 <- maritl == levels(as.factor(maritl))[2]
fit1 <- lm(wage~lvl2)
fit2 <- lm(wage~lvl2+lvl1)
fit3 <- lm(wage~maritl)
anova(fit1,fit2,fit3)
```
The anova test confirms our intuitions: there's no statistically significant difference between Widowed Divorced and Separated, however there is a difference between these three together and Married or Never Married. 

Let's now look at education: 
```{r}
plot(education, wage)
```
If we do the same we've done before for marital status here to education we'll probably obtain opposite results: all the categories are statistically significantly different. But let's put it into numbers. 
```{r}
edu1 <- education == levels(as.factor(education))[1]
edu3 <- education == levels(as.factor(education))[3]
edu5 <- education == levels(as.factor(education))[5]
fit1 <- lm(wage~edu3)
fit2 <- lm(wage~edu3+edu1)
fit3 <- lm(wage~edu3+edu1+edu5)
fit4 <- lm(wage~education)
anova(fit1,fit2,fit3,fit4)
```
Our intution was right. 
Let's now turn to the variable age again, but this time let's use splines.
```{r}
library(gam)
poly.fit <- glm(wage~poly(age,4), data=Wage)
spline.fit <- glm(wage~bs(age, df = 6))
nat.spline.fit <- glm(wage~ns(age, df = 6))
plot(age,wage, col="darkgray")
preds.poly <- predict(poly.fit, newdata=data.frame(age=age.grid))
preds.spline <- predict(spline.fit, newdata=data.frame(age=age.grid))
preds.nat.spline <- predict(nat.spline.fit, newdata=data.frame(age=age.grid))
lines(age.grid, preds.poly, lwd=2, col="blue")
lines(age.grid, preds.spline, lwd=2, col="red")
lines(age.grid, preds.nat.spline, lwd=2, col="darkgreen")
```
Note that the natural spline and the polynomial fit are almost identical. 
Due to its robustness at the boundary we choose the natural spline, and proceed to fit a generalized additive model. 
```{r}
gam.fit <- gam(wage~ns(age, df = 6)+education+lvl2+lvl1)
summary(gam.fit)
par(mfrow=c(2,2))
plot.Gam(gam.fit)
```

# The curse of dimensionality

We're now going to practically face what is known as *the curse of dimensionality* in the context of three different linear models: Knn, multiple linear regression and GAMs. 
We will consider the specific case when the underlying signal is sparse (only one predictor has an effect on the response) and the signal is non-linear.

```{r}
library(gam)
library(FNN)
# Generating artificial dataset
set.seed(1)
# Train set 
xtrain <- matrix(rnorm(20*100),ncol=20, nrow = 100) 
# Note: dimensionality of predictors = 20
ytrain <- sin(2*xtrain[,1]) + 0.3*rnorm(100)
dtrain <- data.frame(xtrain,y = ytrain)
# Test set 
xtest <- matrix(rnorm(20*100),ncol=20, nrow = 100)
ytest <- sin(2*xtest[,1]) + 0.3*rnorm(100)
dtest <- data.frame(xtest,y = ytest)
```
```{r}
head(dtrain)
```

We'll now fit on the training set each of the three models (KNN, multiple linear regression and GAM with splines) with only the first predictor, and then evaluate their test mean squared error. 
```{r}
# KNN first 
# 10 different number of neighbors 
n.ns <- 1:10 
res.knn <- numeric(length(n.ns))
for(n.n in n.ns){
  # Fit KNN model with n.n number of neighbors
  fit <- knn.reg(train=as.matrix(xtrain[,1]), 
             test=as.matrix(xtest[,1]), 
             y=ytrain, k = n.n)
  error <- sum((fit$pred-ytest)**2)/length(ytest)
  res.knn[n.n] <- error
}
plot(n.ns, res.knn, type="l", lty="dashed", lwd=2, col="red")
min.nn <- which.min(res.knn)
points(min.nn, res.knn[min.nn], cex=2)
```
```{r}
#saving best KNN model
fit.knn <- knn.reg(train=as.matrix(xtrain[,1]), 
             test=as.matrix(xtest[,1]), 
             y=ytrain, k = 4)
```

```{r}
# Now multiple linear regression 
lm.fit <- lm(y~X1, data=dtrain)
preds.lm <- predict(lm.fit, dtest)
error <- mean((preds.lm-dtest$y)^2)
error
```


```{r}
# And finally let's use splines 
GAM.fit <- gam(y~s(X1, df = 4), data=dtrain)
preds.GAM <- predict(GAM.fit,dtest)
error <- mean((preds.GAM-dtest$y)^2)
error
```
```{r}
#Plotting 
plot(xtest[,1], ytest, col="darkgray", xlab="x",ylab="y")
points(xtest[,1], preds.lm, col="blue")
points(xtest[,1], preds.GAM, col="red")
points(xtest[,1], fit.knn$pred, col="darkgreen")
```
As we may have expected, the two most flexible models give the best results, while linear regression struggles to fit the data. Now what happens if we increase the dimensionality? 
```{r}
# For each additional predictor we'll record the test MSE
res.dim <- matrix(nrow=19, ncol=12)
predictors <- names(dtrain)[1:19]
for(d in 1:19){
  # FIRST KNNs
  for(n.n in n.ns){
  # Fit KNN model with n.n number of neighbors
  fit <- knn.reg(train=as.matrix(xtrain[,1:d]), 
             test=as.matrix(xtest[,1:d]), 
             y=ytrain, k = n.n)
  error <- mean((fit$pred-ytest)**2)
  res.dim[d, n.n] <- error
  }
  # multiple linear regression
  lm.formula <- as.formula(paste("y~",paste(predictors[1:d],collapse="+"), sep = ""))
  lm.fit <- lm(lm.formula, data=dtrain)
  preds.lm <- predict(lm.fit, dtest)
  res.dim[d,11] <- mean((preds.lm-dtest$y)^2)
  # GAMS
  gams.formula <- as.formula(paste("y~s(",paste(predictors[1:d],
                                                collapse=",4)+s("),",4)",sep = ""))
  GAM.fit <- gam(gams.formula, data=dtrain)
  preds.GAM <- predict(GAM.fit,dtest)
  res.dim[d,12] <- mean((preds.GAM-dtest$y)^2)
}
```

```{r}
res.dim <- data.frame(res.dim)
names(res.dim)<- c("knn1","knn2","knn3","knn4","knn5","knn6","knn7","knn8","knn9","knn10","lm","GAM")
head(res.dim)
```
```{r}
plot(1:19, res.dim$lm, type="l", col="blue", lty="dashed", ylim=c(0.12,0.8), ylab="Test MSE", xlab="Dimensions in the model")
lines(1:19, res.dim$GAM, type="l", col="red", lty="dashed")
lines(1:19, res.dim$knn1, type="l", col=6, lty="dashed")
lines(1:19, res.dim$knn4, type="l", col=11, lty="dashed")
lines(1:19, res.dim$knn7, type="l", col=8, lty="dashed")
lines(1:19, res.dim$knn10, type="l", col=9, lty="dashed")
legend(15,0.6, legend=c("lm","GAM","knn1","knn4","knn7","knn10"), lty="dashed", col=c("blue", "red",6,11,8,9))
```
It is clear from the above results that the KNN approach suffers from the curse of dimensionality the most, while both multiple linear regression and GAMs models slowly increase with the dimension. 


# The backfitting algorithm 
We'll now focus on a topic specific to GAMs, which is that of model fitting. GAMs do not always present a closed form solution (for instance in presence of splines or loess curves). To fit these models the *backfitting algorithm* can be used. 
We'll now implement it for a multiple regression model, which can be seen as a special cas of a GAM.
```{r}
set.seed(1)
# Generating 2 artificial datasets 
# One with completely indipendent variables 
# One with dependent variables
n <- 100
x1 <- rnorm(n)
x2 <- rnorm(n)
x3 <- rnorm(n)
x4 <- rnorm(n)
y <- 0.7 + x1 + 2*x2 + 0.5*x3 -3*x4 + rnorm(n)
# second dataset
x1_cor <- rnorm(n)
x2_cor <- rnorm(n)
x3_cor <- rnorm(n)
x4_cor <- x1_cor + 0.4*rnorm(n)
y_cor <- x1_cor + 2*x2_cor + 0.5*x3_cor -3*x4_cor + rnorm(n)
```
```{r}
niter <- 10 
nvar <- 4 
# one step (one variable modified)
backfit.onestep<-function(xj, y,j, gs){
  other.gs <- gs[,setdiff(1:dim(gs)[2],j)]
  new.y <- y - mean(y) - rowSums(other.gs)
  fitj <- lm(new.y~xj)
  return(fitj)
}
do.backfit <- function(x,y,niter, eps){
  # Initialization 
  fits <- matrix(nrow=nvar, ncol=2)
  gs <- matrix(rep(0, n*nvar), nrow=n, ncol=nvar)
  # Iterations
  for(i in 1:niter){
    old.gs <- gs 
    for(j in 1:dim(x)[2]){
      new.fitj <- backfit.onestep(x[,j],y,j, gs)
      fits[j,] <- new.fitj$coefficients
      gs[,j]<-new.fitj$fitted.values
    }
    # check for convergence
    conv <- all(abs(old.gs - gs) < eps)
    if(conv){
      break
      }
  }
  if(conv){print(paste("Converged after ",paste(i,"iterations")))}
  else{print(paste("Not converged: update = ",max(abs(old.gs - gs))))}
  return(fits)
}
```
```{r}
x <- cbind(x1,x2,x3,x4)
x.cor <- cbind(x1_cor, x2_cor, x3_cor, x4_cor)
x.fits <- do.backfit(x,y,niter=10,eps=0.05)
x.cor.fits <- do.backfit(x.cor,y_cor,niter=10,eps=0.05)
```
Let's give the correlation data more iterations:
```{r}
x.cor.fits <- do.backfit(x.cor,y_cor,niter=100,eps=0.05)
```
Let's now compare the estimate we get from backfitting with those from standard multiple linear regression: we expect to see no difference for the indipendent dimensions data, while we expect to see differences with the second dataset.
```{r}
lm.fit.x <- lm(y~x)
lm.fit.x.cor <- lm(y_cor~x.cor)
```
```{r}
beta0.x <- sum(x.fits[,1])+mean(y)
beta0.x
x.fits[,2]
summary(lm.fit.x)
```
We were right about the indipendent dimensions dataset. What about the second one?
```{r}
beta0.xcor <- sum(x.cor.fits[,1])+mean(y)
beta0.xcor
x.cor.fits[,2]
summary(lm.fit.x.cor)
```
Note how only the estimate of x1 and x4 are off: the reason for this is that x1 and x4 are correlated, while x2 and x3 are indipendend from all the others.


# Tree 
Finally we'll turn to a kind of models completely different from what we've seen so far: trees. 
We'll now apply a Tree to the Carseats dataset to predict the value of Sales given the other variables. 
```{r}
library(ISLR)
attach(Carseats)
head(Carseats)
```
We'll first fit a regression tree to the data, using a validation set approach to estimate its test performance. 
```{r}
library(tree)
# Let's first split the data into training and test sets 
n <- dim(Carseats)[1]
indices <- sample(1:n, size=n, replace=F)
test <- indices[1:n/3]
length(test)
```
```{r}
tree.full <- tree(Sales~., data=Carseats, subset=-test)
plot(tree.full)
text(tree.full, pretty=0)
```
```{r}
summary(tree.full)
```
We have used only 5 of the 10 predictors available, obtaining 17 terminal nodes in total. What test MSE do we get with this fully grown tree?
```{r}
preds <- predict(tree.full, newdata = Carseats[test,])
test.MSE <- mean((Carseats$Sales[test]-preds)^2)
test.MSE
```
AN MSE of practically 4, which means we're on average off by 2000$ on each prediction. Let's see if we can lower this error using pruning. 

To determine the optimal pruning level we use cross validation on our fully grown tree.
```{r}
cv.res <- cv.tree(tree.full, K = 10)
plot(cv.res$size, cv.res$dev, main = "Cross validation error wrt model size", type="b", col="red", xlab="model size")
```
```{r}
cv.res$size[which.min(cv.res$dev)]
```
The optimal size is 13 leaf nodes, according to the cross validation results. Let's use this pruned tree to make predictions. 
```{r}
tree.pruned <- prune.tree(tree.full, best = 13)
plot(tree.pruned)
text(tree.pruned, pretty=0)
```
Let's look at the Test MSE for this pruned version. 
```{r}
preds.pruned <- predict(tree.pruned, newdata = Carseats[test, ])
error.pruned <- mean((Carseats$Sales[test]-preds.pruned)^2)
error.pruned
```
The test MSE increased by 0.5, hence the pruning didn't seems to improve our model. 
Let's now refer to more robust methods such as bagging and random forests to analyze this data.
```{r}
library(randomForest)
tree.bagged <- randomForest(formula=Sales~., data =Carseats, mtry=10, subset = -test)
tree.bagged
```
The mean of squared residuals we see in the summary is obtained by evaluating the performance on the OOB samples for each bootstrapped dataset. If the estimate is reliable the bagged version has almost halved our error. Let's look at the test MSE. 
```{r}
preds.bagged <- predict(tree.bagged, newdata = Carseats[test,])
error.bagged <- mean((Carseats$Sales[test]-preds.bagged)^2)
error.bagged
```
Indeed the OOB estimation of the error is in line with the test MSE: bagging has decreased the error from approximately 4 to approximately 2.3.
One problem with bagging is that the predictors tend to be highly correlated. A simple way to solve this is to allow each split to be done on a random subset of the predictors: this is the random forest approach. 
```{r}
# by default mtry ) p/3 : almost 4 in our case
tree.rf <- randomForest(formula=Sales~., data =Carseats, subset = -test)
tree.rf
```
The OOB estimate seems to be increased wrt bagging. What is this telling us? As the main distinctive feature of trees is the splitting on a random subset, this could mean that the available features at each cut are not enough to lead to signigicant improvements in the model. This can happen when there's not much overlapping between the dimensions in the dataset, while only few of them are predictive. 
In order to prove this reasoning we look at the variable importance, which is an analysis tools available in random forests. 
```{r}
varImpPlot(tree.rf)
```
The plot above seem to validate our resoning. There are only two variables that significantly determine the model performance. This leads naturally to poorer estimates when these 2 variables are not "randomly selected" for the split.

Let's look at the test MSE as we've done for the previous cases to conclude our analysis. 
```{r}
pred.rf <- predict(tree.rf, newdata = Carseats[test,])
error.rf <- mean((Carseats$Sales[test]-pred.rf)^2)
error.rf
```


